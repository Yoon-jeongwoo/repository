{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1FtchADDGLDNl9_7IUsEbquUOg5XBhR5m",
      "authorship_tag": "ABX9TyPvQn4eHdO2zy3VFwxP7asp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yoon-jeongwoo/repository/blob/main/%EC%9E%90%EC%97%B0%EC%96%B4%EB%A5%BC_%ED%99%9C%EC%9A%A9%ED%95%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k7cBgMvTLQbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-17mpgZTUpno",
        "outputId": "42295484-1673-4a48-dccd-9cc0c3448583"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import openai\n",
        "import re"
      ],
      "metadata": {
        "id": "APmwk6hHUrTE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# CSV 파일 불러오기\n",
        "file1_path = '/content/drive/MyDrive/twcs/sample.csv'\n",
        "file2_path = '/content/drive/MyDrive/twcs/twcs.csv'\n",
        "df1 = pd.read_csv(file1_path)\n",
        "df2 = pd.read_csv(file2_path)\n",
        "\n",
        "# response_tweet_id와 in_response_to_tweet_id 열의 아이디 추출\n",
        "response_tweet_ids_df1 = df1['response_tweet_id'].unique()\n",
        "in_response_to_tweet_ids_df1 = df1['in_response_to_tweet_id'].unique()\n",
        "\n",
        "response_tweet_ids_df2 = df2['response_tweet_id'].unique()\n",
        "in_response_to_tweet_ids_df2 = df2['in_response_to_tweet_id'].unique()\n",
        "\n",
        "# NaN 값을 0으로 대체\n",
        "response_tweet_ids_df1 = pd.Series(response_tweet_ids_df1).fillna(0)\n",
        "in_response_to_tweet_ids_df1 = pd.Series(in_response_to_tweet_ids_df1).fillna(0)\n",
        "\n",
        "response_tweet_ids_df2 = pd.Series(response_tweet_ids_df2).fillna(0)\n",
        "in_response_to_tweet_ids_df2 = pd.Series(in_response_to_tweet_ids_df2).fillna(0)\n",
        "\n",
        "# 아이디별 개수 카운트를 위한 딕셔너리 초기화\n",
        "response_tweet_id_counts_df1 = {}\n",
        "in_response_to_tweet_id_counts_df1 = {}\n",
        "\n",
        "response_tweet_id_counts_df2 = {}\n",
        "in_response_to_tweet_id_counts_df2 = {}\n",
        "\n",
        "# response_tweet_id 열의 아이디별 개수 카운트\n",
        "for tweet_id in response_tweet_ids_df1:\n",
        "    count = df1['response_tweet_id'].value_counts()[tweet_id]\n",
        "    response_tweet_id_counts_df1[tweet_id] = count\n",
        "\n",
        "for tweet_id in response_tweet_ids_df2:\n",
        "    count = df2['response_tweet_id'].value_counts()[tweet_id]\n",
        "    response_tweet_id_counts_df2[tweet_id] = count\n",
        "\n",
        "# in_response_to_tweet_id 열의 아이디별 개수 카운트\n",
        "for tweet_id in in_response_to_tweet_ids_df1:\n",
        "    count = df1['in_response_to_tweet_id'].value_counts()[tweet_id]\n",
        "    in_response_to_tweet_id_counts_df1[tweet_id] = count\n",
        "\n",
        "for tweet_id in in_response_to_tweet_ids_df2:\n",
        "    count = df2['in_response_to_tweet_id'].value_counts()[tweet_id]\n",
        "    in_response_to_tweet_id_counts_df2[tweet_id] = count\n",
        "\n",
        "# 결과 출력\n",
        "print(\"File 1 - response_tweet_id 카운트:\")\n",
        "print(response_tweet_id_counts_df1)\n",
        "\n",
        "print(\"\\nFile 1 - in_response_to_tweet_id 카운트:\")\n",
        "print(in_response_to_tweet_id_counts_df1)\n",
        "\n",
        "print(\"\\nFile 2 - response_tweet_id 카운트:\")\n",
        "print(response_tweet_id_counts_df2)\n",
        "\n",
        "print(\"\\nFile 2 - in_response_to_tweet_id 카운트:\")\n",
        "print(in_response_to_tweet_id_counts_df2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-ryPZozhr8yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 데이터 불러오기\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/twcs/sample.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/twcs/twcs.csv')\n",
        "\n",
        "# 필요한 열 선택\n",
        "df1 = df1[['text']]\n",
        "df2 = df2[['text']]\n",
        "\n",
        "# 두 개의 데이터프레임을 합치기\n",
        "df = pd.concat([df1, df2], ignore_index=True)\n",
        "\n",
        "# 랜덤 샘플링으로 데이터 양 조정\n",
        "sampled_df = df.sample(n=10000, random_state=42)\n",
        "\n",
        "# 감정분석 레이블 설정\n",
        "sampled_df['sentiment'] = '중립'  # 중립으로 일단 초기화\n",
        "\n",
        "# 감정분석에 사용할 문장 목록\n",
        "positive_keywords = ['행복', '기쁨', '즐거움', '사랑', '감사']\n",
        "negative_keywords = ['슬픔', '우울', '지루함', '짜증', '분노']\n",
        "\n",
        "# 키워드에 따라 감정분석 레이블 설정\n",
        "for idx, row in sampled_df.iterrows():\n",
        "    text = row['text']\n",
        "    for keyword in positive_keywords:\n",
        "        if keyword in text:\n",
        "            sampled_df.at[idx, 'sentiment'] = '긍정'\n",
        "            break\n",
        "    for keyword in negative_keywords:\n",
        "        if keyword in text:\n",
        "            sampled_df.at[idx, 'sentiment'] = '부정'\n",
        "            break\n",
        "\n",
        "# 데이터셋 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(sampled_df['text'], sampled_df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# 벡터화\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# 분류 모델 학습\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# 테스트 데이터로 예측\n",
        "y_pred = model.predict(X_test_vectorized)\n",
        "\n",
        "# 정확도 평가\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"정확도: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoKBYcAEyj2W",
        "outputId": "595245b5-a080-4a0c-b9c1-dd71d79a3d87"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정확도: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "\n",
        "# CSV 파일 불러오기 (전체 데이터 사용)\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/twcs/sample.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/twcs/twcs.csv')\n",
        "\n",
        "# 샘플링 개수 설정 (전체 데이터를 사용하려면 n=len(df1) 또는 n=len(df2)로 설정)\n",
        "num_samples = 10000\n",
        "\n",
        "# 테스트를 위해 데이터 샘플링 (중복을 허용하여 샘플링)\n",
        "df1_sampled = df1.sample(n=num_samples, replace=True)\n",
        "df2_sampled = df2.sample(n=num_samples, replace=True)\n",
        "\n",
        "# 텍스트 데이터 추출\n",
        "text_data1 = df1_sampled['text'].tolist()\n",
        "text_data2 = df2_sampled['text'].tolist()\n",
        "\n",
        "# TF-IDF 벡터화\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix1 = vectorizer.fit_transform(text_data1)\n",
        "tfidf_matrix2 = vectorizer.transform(text_data2)\n",
        "\n",
        "# 코사인 유사도 계산\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix1, tfidf_matrix2)\n",
        "\n",
        "# 유사도 행렬 출력\n",
        "print(similarity_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usKviDlx1Agx",
        "outputId": "63c4a44f-a301-4214-fcad-a891b530e3b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.02663324 0.         0.         ... 0.         0.         0.01841711]\n",
            " [0.10334961 0.         0.         ... 0.09784552 0.07391343 0.        ]\n",
            " [0.10156838 0.         0.         ... 0.09615915 0.07263953 0.        ]\n",
            " ...\n",
            " [0.02198646 0.         0.         ... 0.         0.0577396  0.01520382]\n",
            " [0.07758877 0.         0.         ... 0.01850924 0.01601298 0.10442264]\n",
            " [0.02357349 0.02703565 0.         ... 0.01363411 0.09152957 0.01630127]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 대화 데이터 샘플\n",
        "conversation_data = df2_sampled[['text', 'response_tweet_id']]\n",
        "\n",
        "# 챗봇 함수\n",
        "def chatbot(input_text, conversation_data):\n",
        "    # 입력 문장의 TF-IDF 벡터화\n",
        "    input_vector = vectorizer.transform([input_text])\n",
        "\n",
        "    # 유사도 계산\n",
        "    similarity_scores = cosine_similarity(input_vector, tfidf_matrix2)\n",
        "\n",
        "    # 가장 유사한 대화 선택\n",
        "    max_similarity_idx = similarity_scores.argmax()\n",
        "    response = conversation_data.iloc[max_similarity_idx]['response_tweet_id']\n",
        "\n",
        "    return response\n",
        "\n",
        "# 챗봇 실행\n",
        "print(\"챗봇: 안녕하세요! 무엇을 도와드릴까요?\")\n",
        "while True:\n",
        "    user_input = input(\"사용자: \")\n",
        "    if user_input == \"안녕\":\n",
        "        print(\"챗봇: 안녕히 가세요!\")\n",
        "        break\n",
        "    response = chatbot(user_input, conversation_data)\n",
        "    print(f\"챗봇: {response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTQCo8GI1VJQ",
        "outputId": "c873fd25-46d3-4dab-c9c6-ed40eb27e2bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "챗봇: 안녕하세요! 무엇을 도와드릴까요?\n",
            "사용자: ㅇㄹ\n",
            "챗봇: 981890\n",
            "사용자: 안녕\n",
            "챗봇: 안녕히 가세요!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 불용어 리스트 정의\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# 데이터 혼합\n",
        "conversation_data = pd.concat([df1[['text', 'in_response_to_tweet_id']], df2[['text', 'in_response_to_tweet_id']]])\n",
        "\n",
        "# 데이터 전처리 함수 (불용어 처리 추가)\n",
        "def preprocess_text(text):\n",
        "    tokens = text.split()\n",
        "    preprocessed_tokens = [token for token in tokens if token not in stop_words]\n",
        "    preprocessed_text = ' '.join(preprocessed_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# 텍스트 전처리\n",
        "conversation_data['text_preprocessed'] = conversation_data['text'].apply(preprocess_text)\n",
        "\n",
        "# 샘플링\n",
        "sample_size = 5000  # 샘플링할 데이터 개수\n",
        "sampled_data = conversation_data.sample(sample_size, random_state=42)\n",
        "\n",
        "# TF-IDF 벡터화\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(sampled_data['text_preprocessed'])\n",
        "\n",
        "# 레이블 인코딩\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(sampled_data['in_response_to_tweet_id'])\n",
        "\n",
        "# 학습 데이터와 테스트 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 구성\n",
        "classifier = SVC(kernel='linear')\n",
        "model = Pipeline([('classifier', classifier)])\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 모델 평가\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"모델 정확도: {accuracy}\")\n",
        "\n",
        "# 챗봇 실행\n",
        "print(\"챗봇: 안녕하세요! 무엇을 도와드릴까요?\")\n",
        "while True:\n",
        "    user_input = input(\"사용자: \")\n",
        "    if user_input == \"안녕\":\n",
        "        print(\"챗봇: 안녕히 가세요!\")\n",
        "        break\n",
        "    # 사용자 입력 전처리\n",
        "    preprocessed_input = preprocess_text(user_input)\n",
        "    # TF-IDF 벡터화\n",
        "    tfidf_input = vectorizer.transform([preprocessed_input])\n",
        "    # 의도 분류\n",
        "    intent = model.predict(tfidf_input)\n",
        "    # 레이블을 텍스트로 변환\n",
        "    response_intent = label_encoder.inverse_transform(intent)[0]\n",
        "    # 분류된 의도에 따라 적절한 응답 출력\n",
        "    response_data = conversation_data[conversation_data['in_response_to_tweet_id'] == response_intent]\n",
        "    if not response_data.empty:\n",
        "        response = response_data['text'].iloc[0]\n",
        "        print(f\"챗봇: {response}\")\n",
        "    else:\n",
        "        print(\"챗봇: 죄송합니다. 제가 도와드릴 수 있는 정보가 없네요.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frN75Lj55CAj",
        "outputId": "2cb8f425-b7f7-4de7-bf07-347487275fa8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 정확도: 0.3\n",
            "챗봇: 안녕하세요! 무엇을 도와드릴까요?\n",
            "사용자: If anything comes up, just let us know.\n",
            "챗봇: 죄송합니다. 제가 도와드릴 수 있는 정보가 없네요.\n",
            "사용자: 안녕\n",
            "챗봇: 안녕히 가세요!\n"
          ]
        }
      ]
    }
  ]
}